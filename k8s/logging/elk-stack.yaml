---
# ===========================================================================
# WAQITI CENTRALIZED LOGGING INFRASTRUCTURE - ELK STACK
# ===========================================================================
# Components:
# - Elasticsearch: Log storage and search
# - Logstash: Log processing and transformation
# - Kibana: Visualization and dashboards
# - Filebeat: Log shipping from pods
# - Fluent Bit: Lightweight log forwarder
# ===========================================================================

---
apiVersion: v1
kind: Namespace
metadata:
  name: logging
  labels:
    name: logging
    monitoring: "true"

---
# Elasticsearch StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    component: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      initContainers:
      - name: increase-vm-max-map
        image: busybox:1.36
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox:1.36
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        env:
        - name: cluster.name
          value: "waqiti-logging"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms2g -Xmx2g"
        - name: xpack.security.enabled
          value: "true"
        - name: xpack.security.transport.ssl.enabled
          value: "true"
        - name: xpack.security.http.ssl.enabled
          value: "false"
        - name: ELASTIC_PASSWORD
          valueFrom:
            secretKeyRef:
              name: elasticsearch-credentials
              key: password
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "6Gi"
            cpu: "2000m"
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        - name: elasticsearch-config
          mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          subPath: elasticsearch.yml
        livenessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 90
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 60
          periodSeconds: 10
      volumes:
      - name: elasticsearch-config
        configMap:
          name: elasticsearch-config
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  ports:
  - name: http
    port: 9200
    targetPort: 9200
  - name: transport
    port: 9300
    targetPort: 9300
  clusterIP: None

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-config
  namespace: logging
data:
  elasticsearch.yml: |
    cluster.name: waqiti-logging
    network.host: 0.0.0.0

    # Index lifecycle management
    xpack.ilm.enabled: true

    # Monitoring
    xpack.monitoring.enabled: true
    xpack.monitoring.collection.enabled: true

    # Security
    xpack.security.enabled: true
    xpack.security.transport.ssl.enabled: true

    # Index settings
    action.auto_create_index: true
    action.destructive_requires_name: true

    # Performance tuning
    indices.memory.index_buffer_size: 30%
    indices.queries.cache.size: 15%
    thread_pool.write.queue_size: 1000
    thread_pool.search.queue_size: 2000

---
# Logstash Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash
  namespace: logging
  labels:
    app: logstash
spec:
  replicas: 3
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
    spec:
      containers:
      - name: logstash
        image: docker.elastic.co/logstash/logstash:8.11.0
        ports:
        - containerPort: 5044
          name: beats
        - containerPort: 9600
          name: monitoring
        env:
        - name: LS_JAVA_OPTS
          value: "-Xms1g -Xmx1g"
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        - name: ELASTICSEARCH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: elasticsearch-credentials
              key: password
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "3Gi"
            cpu: "1000m"
        volumeMounts:
        - name: logstash-config
          mountPath: /usr/share/logstash/pipeline
        - name: logstash-patterns
          mountPath: /usr/share/logstash/patterns
      volumes:
      - name: logstash-config
        configMap:
          name: logstash-config
      - name: logstash-patterns
        configMap:
          name: logstash-patterns

---
apiVersion: v1
kind: Service
metadata:
  name: logstash
  namespace: logging
  labels:
    app: logstash
spec:
  selector:
    app: logstash
  ports:
  - name: beats
    port: 5044
    targetPort: 5044
  - name: monitoring
    port: 9600
    targetPort: 9600

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-config
  namespace: logging
data:
  logstash.conf: |
    input {
      beats {
        port => 5044
        ssl => false
      }

      tcp {
        port => 5000
        codec => json
      }
    }

    filter {
      # Parse JSON logs
      if [message] =~ /^{.*}$/ {
        json {
          source => "message"
        }
      }

      # Add Kubernetes metadata
      if [kubernetes] {
        mutate {
          add_field => {
            "k8s_namespace" => "%{[kubernetes][namespace]}"
            "k8s_pod" => "%{[kubernetes][pod][name]}"
            "k8s_container" => "%{[kubernetes][container][name]}"
          }
        }
      }

      # Parse payment service logs
      if [service] == "payment-service" {
        grok {
          match => {
            "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:thread}\] %{DATA:logger} - %{GREEDYDATA:log_message}"
          }
        }

        # Extract payment IDs
        if [log_message] =~ /paymentId[=:]/ {
          grok {
            match => {
              "log_message" => "paymentId[=:]%{UUID:payment_id}"
            }
          }
        }

        # Extract user IDs
        if [log_message] =~ /userId[=:]/ {
          grok {
            match => {
              "log_message" => "userId[=:]%{UUID:user_id}"
            }
          }
        }

        # Extract transaction amounts
        if [log_message] =~ /amount[=:]/ {
          grok {
            match => {
              "log_message" => "amount[=:]%{NUMBER:amount:float}"
            }
          }
        }
      }

      # Parse fraud detection logs
      if [service] == "fraud-detection-service" {
        if [log_message] =~ /fraudScore[=:]/ {
          grok {
            match => {
              "log_message" => "fraudScore[=:]%{NUMBER:fraud_score:int}"
            }
          }
        }

        # High fraud score tagging
        if [fraud_score] and [fraud_score] > 70 {
          mutate {
            add_tag => ["high_fraud_risk"]
          }
        }
      }

      # Parse error logs
      if [level] == "ERROR" {
        mutate {
          add_tag => ["error"]
        }

        # Extract exception information
        if [log_message] =~ /Exception/ {
          grok {
            match => {
              "log_message" => "%{JAVACLASS:exception_class}[:]? %{GREEDYDATA:exception_message}"
            }
          }
        }
      }

      # Add GeoIP for IP addresses
      if [ip_address] {
        geoip {
          source => "ip_address"
          target => "geoip"
        }
      }

      # Remove sensitive data
      mutate {
        remove_field => ["password", "card_number", "cvv", "ssn"]
      }

      # Add processing timestamp
      mutate {
        add_field => {
          "processed_at" => "%{@timestamp}"
        }
      }
    }

    output {
      elasticsearch {
        hosts => ["${ELASTICSEARCH_HOSTS}"]
        user => "elastic"
        password => "${ELASTICSEARCH_PASSWORD}"
        index => "waqiti-%{service}-%{+YYYY.MM.dd}"

        # Index lifecycle management
        ilm_enabled => true
        ilm_rollover_alias => "waqiti-logs"
        ilm_pattern => "000001"
        ilm_policy => "waqiti-logs-policy"
      }

      # Output high fraud risk events to separate index
      if "high_fraud_risk" in [tags] {
        elasticsearch {
          hosts => ["${ELASTICSEARCH_HOSTS}"]
          user => "elastic"
          password => "${ELASTICSEARCH_PASSWORD}"
          index => "waqiti-fraud-alerts-%{+YYYY.MM.dd}"
        }
      }

      # Output errors to separate index
      if "error" in [tags] {
        elasticsearch {
          hosts => ["${ELASTICSEARCH_HOSTS}"]
          user => "elastic"
          password => "${ELASTICSEARCH_PASSWORD}"
          index => "waqiti-errors-%{+YYYY.MM.dd}"
        }
      }

      # Monitoring
      stdout {
        codec => rubydebug
      }
    }

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-patterns
  namespace: logging
data:
  payment-patterns: |
    PAYMENT_ID [a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}
    REFERENCE_NUM REF-[A-Z0-9]{10}
    AMOUNT_PATTERN \$?[0-9]+\.[0-9]{2}

---
# Kibana Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:8.11.0
        ports:
        - containerPort: 5601
          name: http
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        - name: ELASTICSEARCH_USERNAME
          value: "elastic"
        - name: ELASTICSEARCH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: elasticsearch-credentials
              key: password
        - name: SERVER_NAME
          value: "waqiti-kibana"
        - name: SERVER_HOST
          value: "0.0.0.0"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 120
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 60
          periodSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  selector:
    app: kibana
  type: LoadBalancer
  ports:
  - name: http
    port: 5601
    targetPort: 5601

---
# Fluent Bit DaemonSet (Log Collection)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: logging
  labels:
    app: fluent-bit
spec:
  selector:
    matchLabels:
      app: fluent-bit
  template:
    metadata:
      labels:
        app: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.2.0
        ports:
        - containerPort: 2020
          name: metrics
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
          requests:
            memory: "100Mi"
            cpu: "100m"
          limits:
            memory: "200Mi"
            cpu: "200m"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc/
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: logging

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - pods
  - nodes
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: logging

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         5
        Daemon        Off
        Log_Level     info
        Parsers_File  parsers.conf

    [INPUT]
        Name              tail
        Path              /var/log/containers/*payment-service*.log
        Parser            docker
        Tag               payment.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On

    [INPUT]
        Name              tail
        Path              /var/log/containers/*fraud-detection*.log
        Parser            docker
        Tag               fraud.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB

    [INPUT]
        Name              tail
        Path              /var/log/containers/*wallet-service*.log
        Parser            docker
        Tag               wallet.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB

    [FILTER]
        Name                kubernetes
        Match               *
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On

    [FILTER]
        Name    modify
        Match   *
        Add     cluster example-production

    [OUTPUT]
        Name            es
        Match           *
        Host            ${FLUENT_ELASTICSEARCH_HOST}
        Port            ${FLUENT_ELASTICSEARCH_PORT}
        Logstash_Format On
        Logstash_Prefix waqiti
        Retry_Limit     False
        Type            _doc

  parsers.conf: |
    [PARSER]
        Name   docker
        Format json
        Time_Key time
        Time_Format %Y-%m-%dT%H:%M:%S.%LZ
        Time_Keep Off

    [PARSER]
        Name   json
        Format json
        Time_Key @timestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%LZ

---
# Elasticsearch Credentials Secret
apiVersion: v1
kind: Secret
metadata:
  name: elasticsearch-credentials
  namespace: logging
type: Opaque
stringData:
  # CONFIGURE_IN_VAULT: Replace with actual Elasticsearch password
  password: "CHANGE_ME_IN_VAULT"
  username: "elastic"
