import React, { useState, useEffect, useRef, useCallback } from 'react';
import {
  Box,
  Button,
  Card,
  CardContent,
  Typography,
  CircularProgress,
  Alert,
  Chip,
  Dialog,
  DialogTitle,
  DialogContent,
  DialogActions,
  IconButton,
  LinearProgress,
  List,
  ListItem,
  ListItemText,
  Paper,
  Slide,
} from '@mui/material';
import MicIcon from '@mui/icons-material/Mic';
import MicOffIcon from '@mui/icons-material/MicOff';
import SendIcon from '@mui/icons-material/Send';
import CancelIcon from '@mui/icons-material/Cancel';
import CheckCircleIcon from '@mui/icons-material/CheckCircle';
import ErrorIcon from '@mui/icons-material/Error';
import VolumeUpIcon from '@mui/icons-material/VolumeUp';
import SettingsIcon from '@mui/icons-material/Settings';
import HelpIcon from '@mui/icons-material/Help';;
import { TransitionProps } from '@mui/material/transitions';
import { toast } from 'react-hot-toast';
import { voicePaymentService } from '@/services/voicePaymentService';
import { paymentService } from '@/services/paymentService';

/**
 * Voice Payment Interface Component
 *
 * FEATURES:
 * - Real-time speech recognition
 * - Natural language payment processing
 * - Multi-language support
 * - Voice authentication
 * - Confirmation workflow
 * - Ambient noise handling
 * - Accessibility support
 *
 * SECURITY:
 * - Voice biometric verification
 * - Transaction confirmation required
 * - Rate limiting
 * - Fraud detection integration
 *
 * @author Waqiti Platform Engineering
 * @since 1.0
 */

interface VoiceCommand {
  id: string;
  transcript: string;
  confidence: number;
  intent: string;
  entities: Record<string, any>;
  timestamp: Date;
}

interface ParsedPayment {
  recipient: string;
  recipientId?: string;
  amount: number;
  currency: string;
  note?: string;
  confidence: number;
}

const Transition = React.forwardRef(function Transition(
  props: TransitionProps & {
    children: React.ReactElement<any, any>;
  },
  ref: React.Ref<unknown>,
) {
  return <Slide direction="up" ref={ref} {...props} />;
});

export const VoicePaymentInterface: React.FC = () => {
  // State management
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');
  const [voiceCommands, setVoiceCommands] = useState<VoiceCommand[]>([]);
  const [parsedPayment, setParsedPayment] = useState<ParsedPayment | null>(null);
  const [confirmationOpen, setConfirmationOpen] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [microphonePermission, setMicrophonePermission] = useState<'granted' | 'denied' | 'prompt'>('prompt');
  const [audioLevel, setAudioLevel] = useState(0);
  const [isSpeaking, setIsSpeaking] = useState(false);

  // Refs
  const recognitionRef = useRef<any>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  // Initialize speech recognition
  useEffect(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      setError('Voice payment is not supported in your browser. Please use Chrome, Edge, or Safari.');
      return;
    }

    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    recognitionRef.current = new SpeechRecognition();

    recognitionRef.current.continuous = true;
    recognitionRef.current.interimResults = true;
    recognitionRef.current.lang = 'en-US';
    recognitionRef.current.maxAlternatives = 3;

    recognitionRef.current.onstart = () => {
      console.log('Voice recognition started');
      setIsListening(true);
      setError(null);
    };

    recognitionRef.current.onresult = async (event: any) => {
      let interim = '';
      let final = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          final += transcript;
        } else {
          interim += transcript;
        }
      }

      setInterimTranscript(interim);

      if (final) {
        setTranscript(final);
        await processVoiceCommand(final, event.results[event.results.length - 1][0].confidence);
      }
    };

    recognitionRef.current.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error);

      switch (event.error) {
        case 'no-speech':
          toast.error('No speech detected. Please speak clearly.');
          break;
        case 'audio-capture':
          setError('Microphone not accessible. Please check permissions.');
          setMicrophonePermission('denied');
          break;
        case 'not-allowed':
          setError('Microphone permission denied. Please allow access.');
          setMicrophonePermission('denied');
          break;
        case 'network':
          setError('Network error. Please check your connection.');
          break;
        default:
          setError(`Speech recognition error: ${event.error}`);
      }

      setIsListening(false);
    };

    recognitionRef.current.onend = () => {
      console.log('Voice recognition ended');
      setIsListening(false);
      setInterimTranscript('');
    };

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      stopAudioLevelMonitoring();
    };
  }, []);

  // Request microphone permission
  const requestMicrophonePermission = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      setMicrophonePermission('granted');

      // Set up audio level monitoring
      audioContextRef.current = new AudioContext();
      analyserRef.current = audioContextRef.current.createAnalyser();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      source.connect(analyserRef.current);
      analyserRef.current.fftSize = 256;

      return true;
    } catch (err) {
      console.error('Microphone permission denied:', err);
      setMicrophonePermission('denied');
      setError('Microphone access is required for voice payments.');
      return false;
    }
  };

  // Monitor audio level
  const monitorAudioLevel = useCallback(() => {
    if (!analyserRef.current) return;

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteFrequencyData(dataArray);

    const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
    const normalized = Math.min(100, (average / 128) * 100);
    setAudioLevel(normalized);

    animationFrameRef.current = requestAnimationFrame(monitorAudioLevel);
  }, []);

  const stopAudioLevelMonitoring = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
  };

  // Process voice command
  const processVoiceCommand = async (text: string, confidence: number) => {
    console.log('Processing voice command:', text, 'Confidence:', confidence);

    if (confidence < 0.5) {
      toast.error('Low confidence in speech recognition. Please repeat clearly.');
      return;
    }

    setIsProcessing(true);

    try {
      // Send to backend NLP service for parsing
      const response = await voicePaymentService.parseVoiceCommand(text);

      const command: VoiceCommand = {
        id: crypto.randomUUID(),
        transcript: text,
        confidence,
        intent: response.intent,
        entities: response.entities,
        timestamp: new Date(),
      };

      setVoiceCommands(prev => [...prev, command]);

      // Handle payment intent
      if (response.intent === 'SEND_PAYMENT' || response.intent === 'PAY') {
        const payment: ParsedPayment = {
          recipient: response.entities.recipient,
          recipientId: response.entities.recipientId,
          amount: response.entities.amount,
          currency: response.entities.currency || 'USD',
          note: response.entities.note,
          confidence: confidence,
        };

        setParsedPayment(payment);
        setConfirmationOpen(true);

        // Speak confirmation
        speakText(
          `I understood: Send ${payment.amount} ${payment.currency} to ${payment.recipient}. Please confirm.`
        );
      } else if (response.intent === 'CHECK_BALANCE') {
        // Handle balance check
        const balance = await voicePaymentService.getBalance();
        speakText(`Your current balance is ${balance.amount} ${balance.currency}.`);
        toast.success(`Balance: ${balance.amount} ${balance.currency}`);
      } else if (response.intent === 'TRANSACTION_HISTORY') {
        speakText('Opening your recent transactions.');
        toast.info('Recent transactions displayed');
      } else {
        speakText(`I didn't understand that command. Please try again.`);
        toast.error('Command not recognized');
      }
    } catch (err: any) {
      console.error('Error processing voice command:', err);
      setError(err.message || 'Failed to process voice command');
      toast.error('Failed to process voice command');
    } finally {
      setIsProcessing(false);
    }
  };

  // Text-to-speech
  const speakText = (text: string) => {
    if ('speechSynthesis' in window) {
      setIsSpeaking(true);
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 1.0;
      utterance.pitch = 1.0;
      utterance.volume = 1.0;
      utterance.lang = 'en-US';

      utterance.onend = () => {
        setIsSpeaking(false);
      };

      window.speechSynthesis.speak(utterance);
    }
  };

  // Start listening
  const startListening = async () => {
    if (microphonePermission !== 'granted') {
      const granted = await requestMicrophonePermission();
      if (!granted) return;
    }

    try {
      recognitionRef.current?.start();
      monitorAudioLevel();
      toast.success('Voice payment activated. Start speaking...');
    } catch (err) {
      console.error('Failed to start recognition:', err);
      toast.error('Failed to start voice recognition');
    }
  };

  // Stop listening
  const stopListening = () => {
    recognitionRef.current?.stop();
    stopAudioLevelMonitoring();
    setAudioLevel(0);
  };

  // Confirm payment
  const confirmPayment = async () => {
    if (!parsedPayment) return;

    setIsProcessing(true);
    setConfirmationOpen(false);

    try {
      // Verify voice biometric (if enabled)
      const biometricVerified = await voicePaymentService.verifyVoiceBiometric();

      if (!biometricVerified) {
        toast.error('Voice verification failed. Please try again.');
        setIsProcessing(false);
        return;
      }

      // Execute payment
      const paymentRequest = {
        recipientId: parsedPayment.recipientId,
        amount: parsedPayment.amount,
        currency: parsedPayment.currency,
        note: parsedPayment.note || 'Voice payment',
        channel: 'VOICE',
      };

      const response = await paymentService.sendMoney(paymentRequest);

      speakText(
        `Payment successful. ${parsedPayment.amount} ${parsedPayment.currency} sent to ${parsedPayment.recipient}.`
      );

      toast.success(`Payment sent successfully! Transaction ID: ${response.transactionId}`);

      // Clear state
      setParsedPayment(null);
      setTranscript('');
    } catch (err: any) {
      console.error('Payment failed:', err);
      speakText('Payment failed. Please try again.');
      toast.error(err.message || 'Payment failed');
    } finally {
      setIsProcessing(false);
    }
  };

  // Cancel payment
  const cancelPayment = () => {
    setConfirmationOpen(false);
    setParsedPayment(null);
    speakText('Payment cancelled.');
    toast.info('Payment cancelled');
  };

  return (
    <Box sx={{ maxWidth: 800, mx: 'auto', p: 3 }}>
      {/* Header */}
      <Box sx={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between', mb: 3 }}>
        <Box>
          <Typography variant="h4" gutterBottom>
            ðŸŽ¤ Voice Payments
          </Typography>
          <Typography variant="body2" color="text.secondary">
            Make payments using voice commands
          </Typography>
        </Box>
        <Box>
          <IconButton size="small" sx={{ mr: 1 }}>
            <Settings />
          </IconButton>
          <IconButton size="small">
            <Help />
          </IconButton>
        </Box>
      </Box>

      {/* Microphone Permission Alert */}
      {microphonePermission === 'denied' && (
        <Alert severity="error" sx={{ mb: 2 }}>
          Microphone access is required for voice payments. Please enable it in your browser settings.
        </Alert>
      )}

      {/* Error Alert */}
      {error && (
        <Alert severity="error" onClose={() => setError(null)} sx={{ mb: 2 }}>
          {error}
        </Alert>
      )}

      {/* Main Voice Interface Card */}
      <Card elevation={3}>
        <CardContent>
          {/* Voice Visualization */}
          <Box
            sx={{
              display: 'flex',
              flexDirection: 'column',
              alignItems: 'center',
              justifyContent: 'center',
              py: 4,
            }}
          >
            {/* Microphone Button */}
            <Box sx={{ position: 'relative', mb: 3 }}>
              <IconButton
                onClick={isListening ? stopListening : startListening}
                disabled={isProcessing || microphonePermission === 'denied'}
                sx={{
                  width: 120,
                  height: 120,
                  bgcolor: isListening ? 'error.main' : 'primary.main',
                  color: 'white',
                  '&:hover': {
                    bgcolor: isListening ? 'error.dark' : 'primary.dark',
                  },
                  transition: 'all 0.3s',
                  boxShadow: isListening ? '0 0 20px rgba(255,0,0,0.5)' : 'none',
                }}
              >
                {isProcessing ? (
                  <CircularProgress size={60} color="inherit" />
                ) : isListening ? (
                  <MicOff sx={{ fontSize: 60 }} />
                ) : (
                  <Mic sx={{ fontSize: 60 }} />
                )}
              </IconButton>

              {/* Audio Level Ring */}
              {isListening && (
                <CircularProgress
                  variant="determinate"
                  value={audioLevel}
                  size={140}
                  thickness={2}
                  sx={{
                    position: 'absolute',
                    top: -10,
                    left: -10,
                    color: 'success.main',
                  }}
                />
              )}
            </Box>

            {/* Status */}
            <Chip
              label={
                isProcessing
                  ? 'Processing...'
                  : isListening
                  ? 'Listening...'
                  : 'Tap to start'
              }
              color={isListening ? 'error' : 'default'}
              sx={{ mb: 2 }}
            />

            {/* Transcript Display */}
            <Box sx={{ width: '100%', minHeight: 80 }}>
              {(transcript || interimTranscript) && (
                <Paper
                  elevation={1}
                  sx={{
                    p: 2,
                    bgcolor: 'grey.50',
                    borderLeft: '4px solid',
                    borderColor: 'primary.main',
                  }}
                >
                  <Typography variant="body1" color="text.primary">
                    {transcript}
                  </Typography>
                  {interimTranscript && (
                    <Typography variant="body2" color="text.secondary" sx={{ fontStyle: 'italic' }}>
                      {interimTranscript}
                    </Typography>
                  )}
                </Paper>
              )}
            </Box>
          </Box>

          {/* Example Commands */}
          <Box sx={{ mt: 3 }}>
            <Typography variant="subtitle2" color="text.secondary" gutterBottom>
              Try saying:
            </Typography>
            <List dense>
              <ListItem>
                <ListItemText
                  primary="â€¢ Send 50 dollars to John"
                  secondary="Make a payment to a contact"
                />
              </ListItem>
              <ListItem>
                <ListItemText
                  primary="â€¢ Check my balance"
                  secondary="View your current balance"
                />
              </ListItem>
              <ListItem>
                <ListItemText
                  primary="â€¢ Show recent transactions"
                  secondary="View transaction history"
                />
              </ListItem>
            </List>
          </Box>
        </CardContent>
      </Card>

      {/* Recent Voice Commands */}
      {voiceCommands.length > 0 && (
        <Card sx={{ mt: 3 }}>
          <CardContent>
            <Typography variant="h6" gutterBottom>
              Recent Commands
            </Typography>
            <List>
              {voiceCommands.slice(-5).reverse().map((cmd) => (
                <ListItem key={cmd.id}>
                  <ListItemText
                    primary={cmd.transcript}
                    secondary={`Intent: ${cmd.intent} | Confidence: ${(cmd.confidence * 100).toFixed(0)}%`}
                  />
                </ListItem>
              ))}
            </List>
          </CardContent>
        </Card>
      )}

      {/* Payment Confirmation Dialog */}
      <Dialog
        open={confirmationOpen}
        TransitionComponent={Transition}
        keepMounted
        onClose={cancelPayment}
        maxWidth="sm"
        fullWidth
      >
        <DialogTitle>
          <Box sx={{ display: 'flex', alignItems: 'center', gap: 1 }}>
            <CheckCircle color="success" />
            Confirm Payment
          </Box>
        </DialogTitle>
        <DialogContent>
          {parsedPayment && (
            <Box>
              <Alert severity="info" sx={{ mb: 2 }}>
                Please review and confirm the payment details
              </Alert>

              <Paper elevation={0} sx={{ p: 2, bgcolor: 'grey.50' }}>
                <Typography variant="body2" color="text.secondary" gutterBottom>
                  Recipient
                </Typography>
                <Typography variant="h6" gutterBottom>
                  {parsedPayment.recipient}
                </Typography>

                <Typography variant="body2" color="text.secondary" gutterBottom sx={{ mt: 2 }}>
                  Amount
                </Typography>
                <Typography variant="h4" color="primary" gutterBottom>
                  {parsedPayment.amount} {parsedPayment.currency}
                </Typography>

                {parsedPayment.note && (
                  <>
                    <Typography variant="body2" color="text.secondary" gutterBottom sx={{ mt: 2 }}>
                      Note
                    </Typography>
                    <Typography variant="body1">{parsedPayment.note}</Typography>
                  </>
                )}

                <Box sx={{ mt: 2 }}>
                  <Chip
                    label={`Confidence: ${(parsedPayment.confidence * 100).toFixed(0)}%`}
                    size="small"
                    color={parsedPayment.confidence > 0.8 ? 'success' : 'warning'}
                  />
                </Box>
              </Paper>

              {isSpeaking && (
                <Box sx={{ display: 'flex', alignItems: 'center', gap: 1, mt: 2 }}>
                  <VolumeUp color="action" />
                  <Typography variant="body2" color="text.secondary">
                    Speaking confirmation...
                  </Typography>
                </Box>
              )}
            </Box>
          )}
        </DialogContent>
        <DialogActions>
          <Button
            onClick={cancelPayment}
            startIcon={<Cancel />}
            disabled={isProcessing}
          >
            Cancel
          </Button>
          <Button
            onClick={confirmPayment}
            variant="contained"
            startIcon={<Send />}
            disabled={isProcessing}
          >
            {isProcessing ? 'Processing...' : 'Confirm & Send'}
          </Button>
        </DialogActions>
      </Dialog>
    </Box>
  );
};

export default VoicePaymentInterface;
