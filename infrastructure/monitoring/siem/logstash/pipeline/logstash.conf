# Logstash Pipeline Configuration for Waqiti SIEM
# Processes logs from multiple sources and enriches with security context

input {
  # Beats input for Filebeat
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
    ssl_key => "/usr/share/logstash/config/certs/logstash.key"
    ssl_verify_mode => "force_peer"
    ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca.crt"]
  }

  # Syslog input for system logs
  syslog {
    port => 5514
    type => "syslog"
  }

  # TCP input for application logs
  tcp {
    port => 5000
    codec => json
    type => "application"
  }

  # Kafka input for event streaming
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["payment-events", "wallet-events", "transaction-events", "security-events"]
    group_id => "logstash-consumer"
    codec => json
    type => "kafka-event"
  }

  # HTTP input for webhooks
  http {
    port => 8080
    codec => json
    type => "webhook"
  }
}

filter {
  # Parse JSON logs
  if [type] == "application" {
    json {
      source => "message"
    }
  }

  # Enrich with GeoIP data
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geo"
      fields => ["country_name", "city_name", "location", "timezone"]
    }

    # Check for suspicious geographic patterns
    if [user_id] {
      elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "user-locations-*"
        query => "user_id:%{[user_id]} AND @timestamp:[now-1h TO now]"
        fields => {
          "geo.country_name" => "previous_country"
        }
      }

      if [previous_country] and [geo][country_name] != [previous_country] {
        mutate {
          add_tag => ["geographic_anomaly", "security_alert"]
          add_field => {
            "alert_type" => "IMPOSSIBLE_TRAVEL"
            "alert_severity" => "HIGH"
          }
        }
      }
    }
  }

  # Parse and enrich payment events
  if [type] == "kafka-event" and [event_type] == "payment" {
    # Extract payment details
    mutate {
      add_field => {
        "payment_amount" => "%{[data][amount]}"
        "payment_currency" => "%{[data][currency]}"
        "payment_method" => "%{[data][method]}"
        "sender_id" => "%{[data][sender_id]}"
        "recipient_id" => "%{[data][recipient_id]}"
      }
    }

    # Check for high-value transactions
    if [payment_amount] {
      ruby {
        code => "
          amount = event.get('payment_amount').to_f
          if amount > 10000
            event.set('high_value_transaction', true)
            event.tag('requires_review')
          end
        "
      }
    }

    # Velocity checking
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "payment-events-*"
      query => "sender_id:%{[sender_id]} AND @timestamp:[now-1h TO now]"
      enable_sort => false
      fields => {
        "@timestamp" => "recent_transactions"
      }
      aggregation_fields => {
        "payment_amount" => "total_recent_amount"
      }
    }

    ruby {
      code => "
        recent_count = event.get('recent_transactions')&.length || 0
        if recent_count > 20
          event.set('velocity_alert', true)
          event.tag('high_velocity')
          event.set('alert_severity', 'MEDIUM')
        end
      "
    }
  }

  # Parse security events
  if [event_category] == "security" {
    # Authentication failures
    if [event_type] == "authentication_failure" {
      metrics {
        meter => "auth_failures"
        add_tag => "metric"
      }

      # Count failures per user
      aggregate {
        task_id => "%{user_id}"
        code => "
          map['failures'] ||= 0
          map['failures'] += 1
          map['first_failure'] ||= event.get('@timestamp')
        "
        push_map_as_event_on_timeout => true
        timeout_task_id_field => "user_id"
        timeout => 300
        timeout_tags => ["auth_failure_aggregate"]
      }
    }

    # Detect brute force attempts
    if "auth_failure_aggregate" in [tags] and [failures] > 5 {
      mutate {
        add_tag => ["brute_force_attempt", "security_incident"]
        add_field => {
          "alert_type" => "BRUTE_FORCE"
          "alert_severity" => "CRITICAL"
          "alert_description" => "Multiple authentication failures detected for user %{user_id}"
        }
      }
    }

    # SQL Injection detection
    if [query] or [request_body] {
      ruby {
        code => "
          patterns = [
            /(\bUNION\b.*\bSELECT\b)/i,
            /(\bOR\b.*=.*)/i,
            /(--|\#|\/\*)/,
            /(\bEXEC\b|\bEXECUTE\b)/i,
            /(\bDROP\b.*\bTABLE\b)/i,
            /(\bINSERT\b.*\bINTO\b)/i,
            /(\bDELETE\b.*\bFROM\b)/i
          ]
          
          query = event.get('query') || ''
          body = event.get('request_body') || ''
          content = query + ' ' + body
          
          patterns.each do |pattern|
            if content.match(pattern)
              event.tag('sql_injection_attempt')
              event.set('alert_type', 'SQL_INJECTION')
              event.set('alert_severity', 'CRITICAL')
              break
            end
          end
        "
      }
    }

    # XSS detection
    if [request_body] or [user_input] {
      ruby {
        code => "
          xss_patterns = [
            /<script[^>]*>.*?<\/script>/i,
            /javascript:/i,
            /on\w+\s*=/i,
            /<iframe/i,
            /<embed/i,
            /<object/i
          ]
          
          input = event.get('request_body') || event.get('user_input') || ''
          
          xss_patterns.each do |pattern|
            if input.match(pattern)
              event.tag('xss_attempt')
              event.set('alert_type', 'XSS_ATTACK')
              event.set('alert_severity', 'HIGH')
              break
            end
          end
        "
      }
    }
  }

  # Enrich with threat intelligence
  if [client_ip] {
    # Check against threat feeds
    translate {
      source => "client_ip"
      target => "threat_intel"
      dictionary_path => "/usr/share/logstash/config/threat_intel.yaml"
      fallback => "clean"
    }

    if [threat_intel] != "clean" {
      mutate {
        add_tag => ["malicious_ip", "security_threat"]
        add_field => {
          "alert_type" => "THREAT_INTEL_HIT"
          "alert_severity" => "CRITICAL"
        }
      }
    }
  }

  # PCI DSS compliance tagging
  if [payment_card_data] or [card_number] or [cvv] {
    mutate {
      add_tag => ["pci_dss_relevant", "sensitive_data"]
      add_field => {
        "compliance_scope" => "PCI_DSS"
      }
    }

    # Mask sensitive data
    mutate {
      gsub => [
        "card_number", "(\d{4})\d{8}(\d{4})", "\1********\2",
        "cvv", ".*", "***"
      ]
    }
  }

  # Add correlation ID for distributed tracing
  if ![correlation_id] {
    uuid {
      target => "correlation_id"
    }
  }

  # Calculate risk score
  ruby {
    code => "
      risk_score = 0
      
      # Factor in various risk indicators
      risk_score += 30 if event.get('tags')&.include?('malicious_ip')
      risk_score += 25 if event.get('tags')&.include?('sql_injection_attempt')
      risk_score += 25 if event.get('tags')&.include?('xss_attempt')
      risk_score += 20 if event.get('tags')&.include?('brute_force_attempt')
      risk_score += 15 if event.get('tags')&.include?('geographic_anomaly')
      risk_score += 10 if event.get('tags')&.include?('high_velocity')
      risk_score += 5 if event.get('high_value_transaction')
      
      event.set('risk_score', risk_score)
      
      if risk_score >= 50
        event.set('risk_level', 'CRITICAL')
      elsif risk_score >= 30
        event.set('risk_level', 'HIGH')
      elsif risk_score >= 15
        event.set('risk_level', 'MEDIUM')
      elsif risk_score > 0
        event.set('risk_level', 'LOW')
      else
        event.set('risk_level', 'NONE')
      end
    "
  }

  # Add timestamp if missing
  if ![timestamp] {
    ruby {
      code => "event.set('timestamp', Time.now.utc.iso8601)"
    }
  }

  # Standardize field names
  mutate {
    rename => {
      "client" => "client_info"
      "msg" => "message"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "port", "_grokparsefailure", "_jsonparsefailure"]
  }
}

output {
  # Primary output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    ssl => true
    cacert => "/usr/share/logstash/config/certs/ca.crt"
    
    # Dynamic index routing based on event type
    index => "%{[@metadata][target_index]}"
    template => "/usr/share/logstash/templates/waqiti-template.json"
    template_name => "waqiti-logs"
    template_overwrite => true
    
    # Use document ID for idempotency
    document_id => "%{correlation_id}"
  }

  # Critical alerts to dedicated index
  if [alert_severity] == "CRITICAL" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      ssl => true
      cacert => "/usr/share/logstash/config/certs/ca.crt"
      index => "critical-alerts-%{+YYYY.MM.dd}"
    }

    # Send to alerting system
    http {
      url => "http://alertmanager:9093/api/v1/alerts"
      http_method => "post"
      format => "json"
      mapping => {
        "labels" => {
          "alertname" => "%{alert_type}"
          "severity" => "%{alert_severity}"
          "service" => "%{service_name}"
        }
        "annotations" => {
          "description" => "%{alert_description}"
          "value" => "%{risk_score}"
        }
      }
    }
  }

  # PCI DSS audit trail
  if "pci_dss_relevant" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      user => "elastic"
      password => "${ELASTIC_PASSWORD}"
      ssl => true
      cacert => "/usr/share/logstash/config/certs/ca.crt"
      index => "pci-dss-audit-%{+YYYY.MM.dd}"
    }
  }

  # Send metrics to Prometheus
  if "metric" in [tags] {
    prometheus {
      port => 9091
    }
  }

  # Archive to S3 for long-term retention
  s3 {
    region => "${AWS_REGION}"
    bucket => "${S3_BUCKET}"
    prefix => "logs/%{+YYYY}/%{+MM}/%{+dd}/"
    time_file => 300
    codec => "json_lines"
  }

  # Real-time streaming for dashboards
  websocket {
    host => "0.0.0.0"
    port => 3232
    codec => json
  }

  # Debug output (disable in production)
  if [@metadata][debug] {
    stdout {
      codec => rubydebug
    }
  }
}